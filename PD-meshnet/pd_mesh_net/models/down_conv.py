import torch
from torch.nn import GroupNorm
import torch.nn.functional as F
from torch_geometric.nn.norm import BatchNorm, InstanceNorm

from pd_mesh_net.nn import (DualPrimalConv, DualPrimalEdgePooling,
                            DualPrimalResConv)


class DualPrimalDownConv(torch.nn.Module):
    r"""Down-convolution module used in the mesh segmenter architecture.
    Consists of a series of `num_skips + 1` blocks of: a dual-primal
    convolutional layer followed by an instance normalization layer, a skip
    connection from the previous block and ReLU activation. All but the first
    convolutional layer have the same input and output resolution.
    A DualPrimalEdgePooling layer is optionally added after the last block.
    Based on analogous model from MeshCNN
    (https://github.com/ranahanocka/MeshCNN/).

    Args:
        in_channels_primal, in_channels_dual (int): Number of input channels in
            the primal/dual convolutional layer respectively.
        out_channels_primal, out_channels_dual (int): Number of output channels
            in the primal/dual convolutional layer respectively.
        heads (int): Number of attention heads associated to each of the primal
            and dual convolutional layers.
        concat_primal, concat_dual (bool): If set to :obj:`False`, the attention
            heads associated respectively to the primal and to the dual
            convolutional layers are averaged instead of concatenated.
            (default: :obj:`True`)
        negative_slope_primal, negative_slope_dual (float): LeakyReLU angle of
            the negative slope, respectively for the layers associated to the
            primal graph and for the layers associated to the dual graph.
            (default: :obj:`0.2`)
        dropout_primal, dropout_dual (float): Dropout probability of the
            normalized attention coefficients which exposes each node,
            respectively of the primal and of the dual convolutional layers, to
            a stochastically sampled neighborhood during training.
            (default: :obj:`0`)
        bias_primal, bias_dual (bool): If set to :obj:`False`, the layers
            associated respectively to the primal and to the dual graph will not
            learn an additive bias. (default: :obj:`False`)
        single_dual_nodes (bool): If True, dual graphs are assumed to have
            single nodes; otherwise, they are assumed to have double nodes. Cf.
            :obj:`pd_mesh_net.utils.GraphCreator`.
        undirected_dual_edges (bool): If True, every directed edge in the dual
            graphs is assumed to have an opposite directed edge; otherwise,
            directed edges in the dual graphs are assumed not to have an
            opposite directed edge. Cf. :obj:`pd_mesh_net.utils.GraphCreator`.
        add_self_loops_to_dual_graph (bool): If set to :obj:`True`,
            regular graph-attention convolutional layers are instantiated as
            dual convolutional layers, thus self-loops are added to the dual
            graph. Furthermore, self-loops are added to dual graphs generated by
            pooling layer. If set to :obj:`False` instead, a modified version of
            the graph-attention convolutional layer is instantiated for the dual
            convolutional layers, with no addition of self-loops to the latter,
            and no self loops are added to the dual graphs generated by the
            pooling layer.
        num_skips (int, optional): Number of consecutive blocks of: a
            DualPrimalConv layer followed instance normalization, a skip
            connection from the previous block, and ReLU activation.
            (default: :obj:`1`)
        num_primal_edges_to_keep (int/None, optional):
            - If not None, the value represents the target number of primal
              edges to keep in the pooling layer.
            At most one of this and the arguments
            `fraction_primal_edges_to_keep` and
            `primal_attention_coeff_threshold` can be not None. If they all
            are None, no pooling layer is inserted in the network.
            (default: :obj:`None`)
        fraction_primal_edges_to_keep (float/None, optional):
            - If not None, the value represents the target fraction of primal
              edges to keep in the pooling layer.
            At most one of this and the arguments `num_primal_edges_to_keep` and
            `primal_attention_coeff_threshold` can be not None. If they all are
            None, no pooling layer is inserted in the network.
            (default: :obj:`None`)
        primal_attention_coeff_threshold (float/None, optional):
            - If not None, the value represents the threshold used in the
              pooling layer to pool primal edges for which the attention
              coefficient is above/below this value, depending on the argument
              `use_decreasing_attention_coefficients`.
            At most one of this and the arguments `num_primal_edges_to_keep` and
            `fraction_primal_edges_to_keep` can be not None. If they all are
            None, no pooling layer is inserted in the network.
            (default: :obj:`None`)
        allow_pooling_consecutive_edges (bool, optional): If True, no
            restrictions are put on the primal edges that can be pooled by the
            optional pooling layer. If False, a primal edge can only be pooled
            if no primal nodes to which it belongs have been pooled previously.
            Setting this argument to False is only compatible with top-K pooling
            (cf. arguments `num_primal_edges_to_keep` and
            `fraction_primal_edges_to_keep`). (default: :obj:`True`)
        aggr_primal_features_pooling (str, optional): Parameter of the optional
            pooling layer. If 'mean'/'add', the feature of each new primal node
            after pooling is obtained by respectively averaging and summing the
            features of the primal nodes that get merged into that node.
            (default: :obj:`mean`)
        aggr_dual_features_pooling (str, optional): Parameter of the optional
            pooling layer. If 'mean'/'add', whenever a new dual node is obtained
            by aggregating multiple previous dual nodes its feature is obtained
            by respectively averaging and summing the features of the previous
            dual nodes. (default: :obj:`mean`)
        use_decreasing_attention_coefficients (bool, optional): When using
            pooling based on an amount of edges (cf. arguments 
            `num_primal_edges_to_keep` and `fraction_primals_edges_to_keep`):
            if True, primal edges are pooled by decreasing magnitude of the
            associated attention coefficients; if False, primal edges are pooled
            by increasing magnitude of the associated attention coefficients.
            When using pooling based on attention-coefficient threshold (cf.
            argument `primal_attention_coeff_threshold`): primal edges are
            pooled if the associated attention coefficients are above the
            threshold - if True - or below the threshold - if False.
            (default: :obj:`True`)
        log_ratio_new_old_primal_nodes (bool, optional): If True, each call to
            the forward method will also return a namedtuple. Its element with
            key `ratio_new_old_primal_nodes` is a tensor of length `num_samples`
            elements - where `num_samples` is the number of samples in the
            batch. The i-th element of the tensor will contain the ratio between
            the number of primal nodes in the new, pooled graph and the number
            of primal nodes before pooling, for the i-th sample in the batch.
            (default: :obj:`True`)
        log_ratio_new_old_primal_edges (bool, optional): If True, each call to
            the forward method will also return a namedtuple. Its element with
            key `ratio_new_old_primal_edges` is a tensor of length `num_samples`
            elements - where `num_samples` is the number of samples in the
            batch. The i-th element of the tensor will contain the ratio between
            the number of primal edges in the new, pooled graph and the number
            of primal edges before pooling, for the i-th sample in the batch.
            (default: :obj:`False`)
        return_old_dual_node_to_new_dual_node (bool, optional): If True, the
            optional pooling layer will return the mapping between the dual
            nodes before the pooling operation and the dual nodes after the
            pooling operation. (default: :obj:`False`)
        return_graphs_before_pooling (bool, optional): If True, each call to the
            forward method will also return two
            `torch_geometric.data.batch.Batch` structures representing the
            graphs before the pooling operation is performed. This may be
            required in the up-convolutional layers (cf.
            `pd_mesh_net.models.DualPrimalUpConv`). Otherwise, the two return
            values will be None. (default: :obj:`False`)

    Attributes:
        pool (pd_mesh_net.nn.pool.DualPrimalEdgePooling, optional): Optional
            dual-primal edge pooling layer inserted at the end of the module,
            after the residual blocks.
        ---
        conv{i} (pd_mesh_net.nn.conv.DualPrimalConv), `i` in
            `{0, ..., self.num_skips}`: `i`-th dual-primal mesh-convolution
            layer. If `i` > 0, follows `self.bn_primal/dual{i - 1}` and has
            `self.out_channels_primal` and `self.out_channels_dual` output
            channels for the primal and for the dual features respectively.
        in{i}_primal/dual (torch_geometric.nn.InstanceNorm, optional), `i` in
            `{0, ..., self.num_skips}`: `i`-th instance-normalization layer,
            respectively for the primal and for the dual features. Follows
            `self.conv{i}` and has `self.out_channels_primal`/
            `self.out_channels_dual` output channels.
    """

    def __init__(self,
                 in_channels_primal,
                 in_channels_dual,
                 out_channels_primal,
                 out_channels_dual,
                 heads,
                 concat_primal,
                 concat_dual,
                 negative_slope_primal,
                 negative_slope_dual,
                 dropout_primal,
                 dropout_dual,
                 bias_primal,
                 bias_dual,
                 single_dual_nodes,
                 undirected_dual_edges,
                 add_self_loops_to_dual_graph,
                 num_skips=1,
                 num_primal_edges_to_keep=None,
                 fraction_primal_edges_to_keep=None,
                 primal_attention_coeff_threshold=None,
                 allow_pooling_consecutive_edges=True,
                 aggr_primal_features_pooling='mean',
                 aggr_dual_features_pooling='mean',
                 use_decreasing_attention_coefficients=True,
                 log_ratio_new_old_primal_nodes=True,
                 log_ratio_new_old_primal_edges=False,
                 return_old_dual_node_to_new_dual_node=False,
                 return_graphs_before_pooling=False):
        super(DualPrimalDownConv, self).__init__()
        # Check on arguments.
        assert (len([
            arg for arg in [
                num_primal_edges_to_keep, fraction_primal_edges_to_keep,
                primal_attention_coeff_threshold
            ] if arg is not None
        ]) <= 1), ("Only one of the arguments `num_primal_edges_to_keep`, "
                   "`fraction_primal_edges_to_keep` and "
                   "`primal_attention_coeff_threshold` can be non-None.")
        self.__should_insert_pooling_layer = False
        if (num_primal_edges_to_keep is not None):
            assert (isinstance(num_primal_edges_to_keep, int))
            self.__should_insert_pooling_layer = True
        if (fraction_primal_edges_to_keep is not None):
            assert (isinstance(fraction_primal_edges_to_keep, float))
            self.__should_insert_pooling_layer = True
        if (primal_attention_coeff_threshold is not None):
            assert (isinstance(primal_attention_coeff_threshold, float))
            self.__should_insert_pooling_layer = True
        if (single_dual_nodes):
            assert (undirected_dual_edges), (
                "The dual-graph configuration with single dual nodes and "
                "directed dual edges is not valid. Please specify a different "
                "configuration.")
        if (self.__should_insert_pooling_layer):
            self.__log_ratio_new_old_primal_nodes = (
                log_ratio_new_old_primal_nodes)
            self.__log_ratio_new_old_primal_edges = (
                log_ratio_new_old_primal_edges)
        else:
            self.__log_ratio_new_old_primal_nodes = False
            self.__log_ratio_new_old_primal_edges = False
        self.__return_graphs_before_pooling = return_graphs_before_pooling

        self.__num_skips = num_skips
        # Initialize module.
        # - First dual-primal mesh-convolutional layer.
        self.conv0 = DualPrimalConv(
            in_channels_primal=in_channels_primal,
            in_channels_dual=in_channels_dual,
            out_channels_primal=out_channels_primal,
            out_channels_dual=out_channels_dual,
            heads=heads,
            concat_primal=concat_primal,
            concat_dual=concat_dual,
            negative_slope_primal=negative_slope_primal,
            negative_slope_dual=negative_slope_dual,
            dropout_primal=dropout_primal,
            dropout_dual=dropout_dual,
            bias_primal=bias_primal,
            bias_dual=bias_dual,
            single_dual_nodes=single_dual_nodes,
            undirected_dual_edges=undirected_dual_edges,
            add_self_loops_to_dual_graph=add_self_loops_to_dual_graph)
        # - First instance-normalization layer.
        out_channels_primal_prev_layer = out_channels_primal
        if (concat_primal):
            out_channels_primal_prev_layer *= heads
        out_channels_dual_prev_layer = out_channels_dual
        if (concat_dual):
            out_channels_dual_prev_layer *= heads
        self.in0_primal = InstanceNorm(out_channels_primal_prev_layer)
        self.in0_dual = InstanceNorm(out_channels_dual_prev_layer)

        for skip_idx in range(self.__num_skips):
            # Instance-normalization layer.
            out_channels_primal_prev_layer = out_channels_primal
            if (concat_primal):
                out_channels_primal_prev_layer *= heads
            out_channels_dual_prev_layer = out_channels_dual
            if (concat_dual):
                out_channels_dual_prev_layer *= heads

            setattr(self, f'in{skip_idx+1}_primal',
                    InstanceNorm(out_channels_primal_prev_layer))
            setattr(self, f'in{skip_idx+1}_dual',
                    InstanceNorm(out_channels_dual_prev_layer))
            # Dual-primal mesh-convolutional layer.
            setattr(
                self, f'conv{skip_idx+1}',
                DualPrimalConv(
                    in_channels_primal=out_channels_primal_prev_layer,
                    in_channels_dual=out_channels_dual_prev_layer,
                    out_channels_primal=out_channels_primal,
                    out_channels_dual=out_channels_dual,
                    heads=heads,
                    concat_primal=concat_primal,
                    concat_dual=concat_dual,
                    negative_slope_primal=negative_slope_primal,
                    negative_slope_dual=negative_slope_dual,
                    dropout_primal=dropout_primal,
                    dropout_dual=dropout_dual,
                    bias_primal=bias_primal,
                    bias_dual=bias_dual,
                    single_dual_nodes=single_dual_nodes,
                    undirected_dual_edges=undirected_dual_edges,
                    add_self_loops_to_dual_graph=add_self_loops_to_dual_graph))
        # Optional pooling layer.
        if (self.__should_insert_pooling_layer):
            self.pool = DualPrimalEdgePooling(
                self_loops_in_output_dual_graph=add_self_loops_to_dual_graph,
                single_dual_nodes=single_dual_nodes,
                undirected_dual_edges=undirected_dual_edges,
                num_primal_edges_to_keep=num_primal_edges_to_keep,
                fraction_primal_edges_to_keep=fraction_primal_edges_to_keep,
                primal_att_coeff_threshold=primal_attention_coeff_threshold,
                allow_pooling_consecutive_edges=allow_pooling_consecutive_edges,
                aggr_primal_features=aggr_primal_features_pooling,
                aggr_dual_features=aggr_dual_features_pooling,
                use_decreasing_attention_coefficient=(
                    use_decreasing_attention_coefficients),
                return_old_dual_node_to_new_dual_node=
                return_old_dual_node_to_new_dual_node,
                log_ratio_new_old_primal_nodes=log_ratio_new_old_primal_nodes,
                log_ratio_new_old_primal_edges=log_ratio_new_old_primal_edges)

    def forward(self, primal_graph_batch, dual_graph_batch,
                primal_edge_to_dual_node_idx_batch):
        r"""Forward pass.

        Args:
            primal_graph_batch (torch.data.batch.Batch): Data structure
                containing the primal graphs in the batch.
            dual_graph_batch (torch.data.batch.Batch): Data structure containing
                the dual graphs in the batch.
            primal_edge_to_dual_node_idx_batch (dict): Dictionary representing
                the associations between primal-graph edges (tuple) and
                dual-graph nodes for all the graphs in the batch, as a single
                dictionary.

        Returns:
            primal_graph_batch (torch_geometric.data.batch.Batch): Output
                primal-graph batch.
            dual_graph_batch (torch_geometric.data.batch.Batch): Output
                dual-graph batch.
            primal_edge_to_dual_node_idx_batch (dict): Output dictionary
                representing the associations between primal-graph edges and
                dual-graph nodes in the batch.
            log_info (pd_mesh_net.nn.pool.PoolingInfo/None): Logging information
                about the optional pooling layer. If no pooling layer is used,
                None. Otherwise, it contains the following attributes:
                - old_primal_node_to_new_one (torch.Tensor): Tensor of length
                    `num_nodes` - where `num_nodes` is the number of nodes in
                    the primal graph before the optional pooling operation. The
                    i-th element of the list will contain the index of the face
                    cluster to which the i-th face in the mesh inputted to the
                    pooling layer belongs after the pooling operation.
                - old_dual_node_to_new_one (torch.Tensor or None): Optionally
                    maps each input dual node to the corresponding dual node
                    after pooling. Input dual nodes with no corresponding output
                    dual nodes are mapped to -1.
                - old_primal_edge_index (torch.Tensor): Primal-graph
                    connectivity matrix before pooling.
                - old_dual_edge_index (torch.Tensor): Dual-graph connectivity
                    matrix before pooling.
                - old_primal_graph_batch (torch.Tensor): Mapping between each
                    primal node and its sample in the batch, before pooling.
                - old_dual_graph_batch (torch.Tensor): Mapping between each
                    dual node and its sample in the batch, before pooling.
                - old_primal_edge_to_dual_node_index (dict): Mapping from primal
                    graph to dual graph before pooling.
                - ratio_new_old_primal_nodes (torch.Tensor): If the class
                    argument `log_ratio_new_old_primal_nodes` is True:
                    - If no logging is set up in the pooling layer, None;
                    - Otherwise, tensor of length `num_samples` - where
                      `num_samples` is the number of samples in the batch. The
                      i-th element of the tensor will contain the ratio between
                      the number of primal nodes in the new, pooled graph and
                      the number of primal nodes before pooling, for the i-th
                      sample in the batch.
                - ratio_new_old_primal_edges (torch.Tensor/None): If the class
                    argument `log_ratio_new_old_primal_edges` is True:
                    - If no pooling is set up in the pooling layer, None;
                    - Otherwise, tensors of length `num_samples` - where
                      `num_samples` is the number of samples in the batch. The
                      i-th element of the tensor will contain the ratio between
                      the number of primal edges in the new, pooled graph and
                      the number of primal edges before pooling, for the i-th
                      sample in the batch.
            primal_graph_batch_before_pooling (torch_geometric.data.batch.Batch/
                None): If the class argument `return_graphs_before_pooling` is
                True, batch containing the primal graphs before pooling.
                Otherwise, None.
            dual_graph_batch_before_pooling (torch_geometric.data.batch.Batch/
                None): If the class argument `return_graphs_before_pooling` is
                True, batch containing the dual graphs before pooling.
                Otherwise, None.
        """
        x_primal, edge_index_primal = (primal_graph_batch.x,
                                       primal_graph_batch.edge_index)
        x_dual, edge_index_dual = (dual_graph_batch.x,
                                   dual_graph_batch.edge_index)

        # First convolution layer.
        x_primal, x_dual = self.conv0(
            x_primal=x_primal,
            x_dual=x_dual,
            edge_index_primal=edge_index_primal,
            edge_index_dual=edge_index_dual,
            primal_edge_to_dual_node_idx=primal_edge_to_dual_node_idx_batch)
        # First instance-normalization layer.
        x_primal = F.relu(self.in0_primal(x_primal))
        x_dual = F.relu(self.in0_dual(x_dual))
        x1_primal = x_primal
        x1_dual = x_dual
        # Remaining convolutions/normalizations/residual connections.
        for skip_idx in range(self.__num_skips):
            # - Convolution.
            x_primal, x_dual = getattr(self, f'conv{skip_idx+1}')(
                x_primal=x_primal,
                x_dual=x_dual,
                edge_index_primal=edge_index_primal,
                edge_index_dual=edge_index_dual,
                primal_edge_to_dual_node_idx=primal_edge_to_dual_node_idx_batch)
            # - Instance normalization.
            x_primal = getattr(self, f'in{skip_idx+1}_primal')(x_primal)
            x_dual = getattr(self, f'in{skip_idx+1}_dual')(x_dual)
            # - Skip connection.
            x_primal = F.relu(x_primal + x1_primal)
            x_dual = F.relu(x_dual + x1_dual)
            x1_primal = x_primal
            x1_dual = x_dual
        # Update the features of the input batch with those from the last
        # convolutional layer.
        primal_graph_batch.x = x_primal
        dual_graph_batch.x = x_dual
        primal_graph_batch_before_pooling = None
        dual_graph_batch_before_pooling = None
        if (self.__return_graphs_before_pooling):
            primal_graph_batch_before_pooling = primal_graph_batch.clone()
            dual_graph_batch_before_pooling = dual_graph_batch.clone()
        # Optional pooling layer.
        if (self.__should_insert_pooling_layer):
            # - Retrieve primal attention coefficients from last convolutional
            #   layer.
            primal_attention_coeffs = getattr(
                self, f'conv{self.__num_skips}').primal_attention_coefficients
            # - Apply pooling.
            (primal_graph_batch, dual_graph_batch,
             primal_edge_to_dual_node_idx_batch, pooling_log) = self.pool(
                 primal_graph_batch=primal_graph_batch,
                 dual_graph_batch=dual_graph_batch,
                 primal_edge_to_dual_node_idx_batch=
                 primal_edge_to_dual_node_idx_batch,
                 primal_attention_coeffs=primal_attention_coeffs)

            log_info = pooling_log
        else:
            log_info = None

        return (primal_graph_batch, dual_graph_batch,
                primal_edge_to_dual_node_idx_batch, log_info,
                primal_graph_batch_before_pooling,
                dual_graph_batch_before_pooling)


class DualPrimalResDownConv(torch.nn.Module):
    r"""Down-convolution module used mainly in the mesh classifier architecture,
    and optionally in the mesh segmenter architecture.
    Consists of a series of blocks made of: a `DualPrimalResConv` block,
    followed by an optional batch-/group- normalization layer and an optional
    `DualPrimalEdgePooling` layer.
    Based on analogous model from MeshCNN
    (https://github.com/ranahanocka/MeshCNN/).

    Args:
        in_channels_primal, in_channels_dual (int): Size of each input sample
            from the primal/dual graph respectively.
        out_channels_primal, out_channels_dual (int): Number of output
            channels of the primal/dual convolutional layer respectively, i.e.,
            dimension of the feature vectors outputted by the primal/dual
            convolutional layer respectively.
        norm_layer_type (str or None): Type of normalization layer to be used
            after the convolutional layer, both primal and dual. Possible
            options are: `None` (no normalization layer), `'group_norm'` (group
            normalization), `'batch_norm'` (batch normalization).
        num_groups_norm_layer (int): If `norm_layer_type` is of 'group_norm',
            number of groups in each normalization layer.
        single_dual_nodes (bool): If True, it will be assumed that the dual
            graphs have single nodes; otherwise, it will be assumed that they
            have double nodes. Cf. :obj:`pd_mesh_net.utils.GraphCreator`.
        undirected_dual_edges (bool): If True, it will be assumed that every
            directed edge in the dual graphs has an opposite directed edge;
            otherwise, it will be assumed that directed edges in the dual graphs
            do not have an opposite directed edge.
            Cf. :obj:`pd_mesh_net.utils.GraphCreator`.
        num_primal_edges_to_keep (int/None, optional):
            - If not None, the value represents the target number of primal
              edges to keep in the pooling layer.
            At most one of this and the arguments
            `fraction_primal_edges_to_keep` and
            `primal_attention_coeffs_threshold` can be not None. If they all
            are None, no pooling layer is inserted in the block.
            (default: :obj:`None`)
        fraction_primal_edges_to_keep (float/None, optional):
            - If not None, the value represents the target
              fraction of primal edges to keep in the pooling layer.
            At most one of this and the arguments `num_primal_edges_to_keep` and
            `primal_attention_coeffs_threshold` can be not None. If they all
            are None, no pooling layer is inserted in the block.
            (default: :obj:`None`)
        primal_attention_coeffs_threshold (float/None, optional):
            - If not None, the value represents the threshold used in the
              pooling layer to pool primal edges for which the attention
              coefficient is above/below this value, depending on the argument
              `use_decreasing_attention_coefficients`.
            At most one of this and the arguments `num_primal_edges_to_keep` and
            `fraction_primal_edges_to_keep` can be not None. If they all are
            None, no pooling layers is inserted in the block.
            (default: :obj:`None`)
        num_res_blocks (int, optional): Number of residual blocks.
            (default: :obj:`3`).
        heads (int, optional): Number of attention heads associated to each of
            the primal and dual convolutional layers. (default: :obj:`1`)
        concat_primal, concat_dual (bool, optional): If set to :obj:`False`, the
            attention heads associated respectively to the primal and to the
            dual convolutional layer are averaged instead of concatenated.
            (default: :obj:`True`)
        negative_slope_primal, negative_slope_dual (float, optional): LeakyReLU
            angle of the negative slope, respectively for the layer associated
            to the primal graph and for the layer associated to the dual graph.
            (default: :obj:`0.2`)
        dropout_primal, dropout_dual (float, optional): Dropout probability of
            the normalized attention coefficients which exposes each node,
            respectively of the primal and of the dual convolutional layer, to a
            stochastically sampled neighborhood during training.
            (default: :obj:`0`)
        bias_primal, bias_dual (bool, optional): If set to :obj:`False`, the
            layer associated respectively to the primal and to the dual graph
            will not learn an additive bias. (default: :obj:`False`)
        add_self_loops_to_dual_graph (bool, optional): If set to :obj:`True`, a
            regular graph-attention convolutional layer is instantiated as dual
            convolutional layer, thus self-loops are added to the dual graph.
            Furthermore, self-loops are added to dual graphs generated by the
            pooling layer. If set to :obj:`False` instead, a modified version of
            the graph-attention convolutional layer is instantiated for the dual
            convolutional layer, with no addition of self-loops to the latter,
            and no self loops are added to the dual graphs generated by the
            pooling layer. (default: :obj:`False`)
        allow_pooling_consecutive_edges (bool, optional): If True, no
            restrictions are put on the primal edges that can be pooled by the
            pooling layer in the block. If False, a primal edge can only be
            pooled if no primal nodes to which it belongs have been pooled
            previously. Setting this argument to False is only compatible with
            top-K pooling (cf. arguments `num_primal_edges_to_keep` and
            `fraction_primal_edges_to_keep`). (default: :obj:`True`)
        aggr_primal_features_pooling (str, optional): Parameter of the optional
            pooling layer. If 'mean'/'add', the feature of each new primal node
            after pooling is obtained by respectively averaging and summing the
            features of the primal nodes that get merged into that node.
            (default: :obj:`mean`)
        aggr_dual_features_pooling (str, optional): Parameter of the optional
            pooling layer. If 'mean'/'add', whenever a new dual node is obtained
            by aggregating multiple previous dual nodes its feature is obtained
            by respectively averaging and summing the features of the previous
            dual nodes. (default: :obj:`mean`)
        use_decreasing_attention_coefficients (bool, optional): When using
            pooling based on an amount of edges (cf. arguments 
            `num_primal_edges_to_keep` and `fractions_primals_edges_to_keep`):
            if True, primal edges are pooled by decreasing magnitude of the
            associated attention coefficients; if False, primal edges are pooled
            by increasing magnitude of the associated attention coefficients.
            When using pooling based on attention-coefficient threshold (cf.
            argument `primal_attention_coeffs_threshold`): primal edges are
            pooled if the associated attention coefficients are above the
            threshold - if True - or below the threshold - if False.
            (default: :obj:`True`)
        log_ratio_new_old_primal_nodes (bool, optional): If True, each call to
            the forward method will also return a namedtuple. Its element with
            key `ratio_new_old_primal_nodes` is a tensor of length `num_samples`
            elements - where `num_samples` is the number of samples in the
            batch. The i-th element of the tensor will contain the ratio between
            the number of primal nodes in the new, pooled graph and the number
            of primal nodes before pooling, for the i-th sample in the batch.
            (default: :obj:`True`)
        log_ratio_new_old_primal_edges (bool, optional): If True, each call to
            the forward method will also return a namedtuple. Its element with
            key `ratio_new_old_primal_edges` is a tensor of length `num_samples`
            elements - where `num_samples` is the number of samples in the
            batch. The i-th element of the tensor will contain the ratio between
            the number of primal edges in the new, pooled graph and the number
            of primal edges before pooling, for the i-th sample in the batch.
            (default: :obj:`False`)
        return_old_dual_node_to_new_dual_node (bool, optional): If True, the
            optional pooling layer will return the mapping between the dual
            nodes before the pooling operation and the dual nodes after the
            pooling operation. (default: :obj:`False`)
        return_graphs_before_pooling (bool, optional): If True, each call to the
            forward method will also return two
            `torch_geometric.data.batch.Batch` structures representing the
            graphs before the pooling operation is performed. This may be
            required in the up-convolutional layers (cf.
            `pd_mesh_net.models.DualPrimalUpConv`). Otherwise, the two return
            values will be None. (default: :obj:`False`)

    Attributes:
        conv (pd_mesh_net.nn.conv.DualPrimalResConv): dual-primal
            mesh-convolution layer.
        norm_primal/dual (torch_geometric.nn.BatchNorm or torch.nn.GroupNorm,
            optional): Batch-/group-normalization layer, respectively for the
            primal and for the dual features.
        pool (pd_mesh_net.nn.pool.DualPrimalEdgePooling, optional): If defined,
            dual-primal edge-pooling layer that follows the normalization layer.
            The output resolution is determined by `num_primal_edges_to_keep`,
            `fraction_primal_edges_to_keep` or
            `primal_attention_coeffs_threshold`.
    """

    def __init__(self,
                 in_channels_primal,
                 in_channels_dual,
                 out_channels_primal,
                 out_channels_dual,
                 norm_layer_type,
                 num_groups_norm_layer,
                 single_dual_nodes,
                 undirected_dual_edges,
                 num_primal_edges_to_keep=None,
                 fraction_primal_edges_to_keep=None,
                 primal_attention_coeffs_threshold=None,
                 num_res_blocks=3,
                 heads=1,
                 concat_primal=True,
                 concat_dual=True,
                 negative_slope_primal=0.2,
                 negative_slope_dual=0.2,
                 dropout_primal=0,
                 dropout_dual=0,
                 bias_primal=False,
                 bias_dual=False,
                 add_self_loops_to_dual_graph=False,
                 allow_pooling_consecutive_edges=True,
                 aggr_primal_features_pooling='mean',
                 aggr_dual_features_pooling='mean',
                 use_decreasing_attention_coefficients=True,
                 log_ratio_new_old_primal_nodes=True,
                 log_ratio_new_old_primal_edges=False,
                 return_old_dual_node_to_new_dual_node=False,
                 return_graphs_before_pooling=False):
        super(DualPrimalResDownConv, self).__init__()
        self.__use_pooling = (num_primal_edges_to_keep is not None or
                              fraction_primal_edges_to_keep is not None or
                              primal_attention_coeffs_threshold is not None)
        self.__norm_layer_type = norm_layer_type

        if (self.__use_pooling):
            self.__log_ratio_new_old_primal_nodes = (
                log_ratio_new_old_primal_nodes)
            self.__log_ratio_new_old_primal_edges = (
                log_ratio_new_old_primal_edges)
        else:
            self.__log_ratio_new_old_primal_nodes = False
            self.__log_ratio_new_old_primal_edges = False
        self.__return_graphs_before_pooling = return_graphs_before_pooling

        # Add the convolution layer.
        self.conv = DualPrimalResConv(
            in_channels_primal=in_channels_primal,
            in_channels_dual=in_channels_dual,
            out_channels_primal=out_channels_primal,
            out_channels_dual=out_channels_dual,
            heads=heads,
            concat_primal=concat_primal,
            concat_dual=concat_dual,
            negative_slope_primal=negative_slope_primal,
            negative_slope_dual=negative_slope_dual,
            dropout_primal=dropout_primal,
            dropout_dual=dropout_dual,
            bias_primal=bias_primal,
            bias_dual=bias_dual,
            single_dual_nodes=single_dual_nodes,
            undirected_dual_edges=undirected_dual_edges,
            add_self_loops_to_dual_graph=add_self_loops_to_dual_graph,
            num_skips=num_res_blocks)
        # Optionally add a normalization layer.
        out_channels_primal_considering_heads = out_channels_primal
        if (concat_primal):
            out_channels_primal_considering_heads *= heads
        out_channels_dual_considering_heads = out_channels_dual
        if (concat_dual):
            out_channels_dual_considering_heads *= heads
        if (norm_layer_type == 'batch_norm'):
            self.norm_primal = BatchNorm(
                in_channels=out_channels_primal_considering_heads)
            self.norm_dual = BatchNorm(
                in_channels=out_channels_dual_considering_heads)
        elif (norm_layer_type == 'group_norm'):
            self.norm_primal = GroupNorm(
                num_groups=num_groups_norm_layer,
                num_channels=out_channels_primal_considering_heads)
            self.norm_dual = GroupNorm(
                num_groups=num_groups_norm_layer,
                num_channels=out_channels_dual_considering_heads)
        # Add pooling layer.
        if (self.__use_pooling):
            self.pool = DualPrimalEdgePooling(
                self_loops_in_output_dual_graph=add_self_loops_to_dual_graph,
                single_dual_nodes=single_dual_nodes,
                undirected_dual_edges=undirected_dual_edges,
                num_primal_edges_to_keep=num_primal_edges_to_keep,
                fraction_primal_edges_to_keep=fraction_primal_edges_to_keep,
                primal_att_coeff_threshold=primal_attention_coeffs_threshold,
                allow_pooling_consecutive_edges=allow_pooling_consecutive_edges,
                aggr_primal_features=aggr_primal_features_pooling,
                aggr_dual_features=aggr_dual_features_pooling,
                use_decreasing_attention_coefficient=(
                    use_decreasing_attention_coefficients),
                return_old_dual_node_to_new_dual_node=
                return_old_dual_node_to_new_dual_node,
                log_ratio_new_old_primal_nodes=log_ratio_new_old_primal_nodes,
                log_ratio_new_old_primal_edges=log_ratio_new_old_primal_edges)

    def forward(self, primal_graph_batch, dual_graph_batch,
                primal_edge_to_dual_node_idx_batch):
        r"""Forward pass.

        Args:
            primal_graph_batch (torch.data.batch.Batch): Data structure
                containing the primal graphs in the batch.
            dual_graph_batch (torch.data.batch.Batch): Data structure containing
                the dual graphs in the batch.
            primal_edge_to_dual_node_idx_batch (dict): Dictionary representing
                the associations between primal-graph edges (tuple) and
                dual-graph nodes for all the graphs in the batch, as a single
                dictionary.

        Returns:
            primal_graph_batch (torch_geometric.data.batch.Batch): Output
                primal-graph batch.
            dual_graph_batch (torch_geometric.data.batch.Batch): Output
                dual-graph batch.
            primal_edge_to_dual_node_idx_batch (dict): Output dictionary
                representing the associations between primal-graph edges and
                dual-graph nodes in the batch.
            log_info (pd_mesh_net.nn.pool.PoolingInfo/None): Logging information
                about the optional pooling layer. If no pooling layer is used,
                None. Otherwise, it contains the following attributes:
                - old_primal_node_to_new_one (torch.Tensor): Tensor of length
                    `num_nodes` - where `num_nodes` is the number of nodes in
                    the primal graph before the optional pooling operation. The
                    i-th element of the list will contain the index of the face
                    cluster to which the i-th face in the mesh inputted to the
                    pooling layer belongs after the pooling operation.
                - old_dual_node_to_new_one (torch.Tensor or None): Optionally
                    maps each input dual node to the corresponding dual node
                    after pooling. Input dual nodes with no corresponding output
                    dual nodes are mapped to -1.                    
                - old_primal_edge_index (torch.Tensor): Primal-graph
                    connectivity matrix before pooling.
                - old_dual_edge_index (torch.Tensor): Dual-graph connectivity
                    matrix before pooling.
                - old_primal_graph_batch (torch.Tensor): Mapping between each
                    primal node and its sample in the batch, before pooling.
                - old_dual_graph_batch (torch.Tensor): Mapping between each
                    dual node and its sample in the batch, before pooling.
                - old_primal_edge_to_dual_node_index (dict): Mapping from primal
                    graph to dual graph before pooling.
                - ratio_new_old_primal_nodes (torch.Tensor): If the class
                    argument `log_ratio_new_old_primal_nodes` is True:
                    - If no logging is set up in the pooling layer, None;
                    - Otherwise, tensor of length `num_samples` - where
                      `num_samples` is the number of samples in the batch. The
                      i-th element of the tensor will contain the ratio between
                      the number of primal nodes in the new, pooled graph and
                      the number of primal nodes before pooling, for the i-th
                      sample in the batch.
                - ratio_new_old_primal_edges (torch.Tensor/None): If the class
                    argument `log_ratio_new_old_primal_edges` is True:
                    - If no pooling is set up in the pooling layer, None;
                    - Otherwise, tensors of length `num_samples` - where
                      `num_samples` is the number of samples in the batch. The
                      i-th element of the tensor will contain the ratio between
                      the number of primal edges in the new, pooled graph and
                      the number of primal edges before pooling, for the i-th
                      sample in the batch.
            primal_graph_batch_before_pooling (torch_geometric.data.batch.Batch/
                None): If the class argument `return_graphs_before_pooling` is
                True, batch containing the primal graphs before pooling.
                Otherwise, None.
            dual_graph_batch_before_pooling (torch_geometric.data.batch.Batch/
                None): If the class argument `return_graphs_before_pooling` is
                True, batch containing the dual graphs before pooling.
                Otherwise, None.
        """
        x_primal, edge_index_primal = (primal_graph_batch.x,
                                       primal_graph_batch.edge_index)
        x_dual, edge_index_dual = (dual_graph_batch.x,
                                   dual_graph_batch.edge_index)

        # Convolution.
        x_primal, x_dual = self.conv(
            x_primal=x_primal,
            x_dual=x_dual,
            edge_index_primal=edge_index_primal,
            edge_index_dual=edge_index_dual,
            primal_edge_to_dual_node_idx=primal_edge_to_dual_node_idx_batch)
        # If available, use normalization layer.
        if (self.__norm_layer_type is not None):
            x_primal = F.relu(self.norm_primal(x_primal))
            x_dual = F.relu(self.norm_dual(x_dual))
        # Update the features of the input batch with those from the last
        # convolutional layer.
        primal_graph_batch.x = x_primal
        dual_graph_batch.x = x_dual
        primal_graph_batch_before_pooling = None
        dual_graph_batch_before_pooling = None
        if (self.__return_graphs_before_pooling):
            primal_graph_batch_before_pooling = primal_graph_batch.clone()
            dual_graph_batch_before_pooling = dual_graph_batch.clone()
        # If available, use primal-attention-driven pooling.
        if (hasattr(self, 'pool')):
            primal_attention_coeffs = self.conv.primal_attention_coefficients
            (primal_graph_batch, dual_graph_batch,
             primal_edge_to_dual_node_idx_batch, pooling_log) = self.pool(
                 primal_graph_batch=primal_graph_batch,
                 dual_graph_batch=dual_graph_batch,
                 primal_edge_to_dual_node_idx_batch=
                 primal_edge_to_dual_node_idx_batch,
                 primal_attention_coeffs=primal_attention_coeffs)

            log_info = pooling_log

        else:
            log_info = None

        return (primal_graph_batch, dual_graph_batch,
                primal_edge_to_dual_node_idx_batch, log_info,
                primal_graph_batch_before_pooling,
                dual_graph_batch_before_pooling)
